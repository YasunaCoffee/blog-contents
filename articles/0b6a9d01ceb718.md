---
title: "はじめての生成AI用パソコン設定～PytorchとTensorFlowがGPUを認識するまで"
emoji: "🐧"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [生成ai, ai, pytorch, tensorflow, wsl, nvidia]
published: true
---
# はじめに
生成AIを勉強するために、はじめてデスクトップWindowsPCを買いました。
今回は、買ったばかりのPCのGPUで生成AIが動かせるように初期設定を行い、PytorchとTensorFlowがGPUを認識するように設定してきます！

# PCスペック
:::message
- CPU:	AMD Ryzen 7-5700X 3.4GHz-4.6GHz(8コア16スレッド)
- メモリー:	32GB(DDR4-PC3200 16GB×2)
- SSD/HDD:	SSD 1TB PCIe Gen4 x4 M.2 NVMe
- HDD 2TB Western Digital WD Red 3.5インチ
- GPU:	ZOTAC NVIDIA GeForce RTX4080 (GDDR6X-16GB)
- OS:	Windows 11 Pro 64bit
:::
このPCに出会うまでのストーリーが詳しく書きました。
https://zenn.dev/yasuna/articles/e574c9154d678d


# 機械学習用ソフトウェアの初期設定
まずは、生成AIを動かすための環境を整備していきましょう。
今回は機械学習用ソフトウェアライブラリ「Pytorch」と「TensorFlow」が使える環境に合わせて設定していきます。

## Pytorchとは元Facebookのグループが開発した機械学習ライブラリ
>PyTorchは、コンピュータビジョンや自然言語処理で利用されているTorchを元に作られた、Pythonのオープンソースの機械学習ライブラリである。最初はFacebookの人工知能研究グループAI Research lab（FAIR）により開発された(Wikiより引用)

https://pytorch.org/

## TensorFlowはGoogleが開発した機械学習ライブラリ
>TensorFlow（テンソルフロー、テンサーフロー）とは、Googleが開発しオープンソースで公開している、機械学習に用いるためのソフトウェアライブラリ(Wikiより引用)

https://www.tensorflow.org

以下の「pip を使用して TensorFlow をインストールする」公式ドキュメントを参考に設定していきましたが、少し導入につまづいてしまいました。
https://www.tensorflow.org/install/pip?hl=ja#windows-wsl2

ソフトウェアがCPUにしか認識されず、困っていたところ
こちらの福山大学の金子先生の記事が大変参考なりましたので、今回はこちらから設定していきます。本当に感謝です！！！
https://www.kkaneko.jp/tools/wsl/wsl_tensorflow2.html

## Windows Subsystem for Linux2(WSL2)の設定
まずはコマンドを実行するシェルを設定していきます。
Windows Subsystem for Linux 2はTensorFlowでサポートされているので採用していきます。
金子先生の記事を参考にしてWindowsにLinux(Ubuntu)をインストールしました。
https://www.kkaneko.jp/tools/wsl/wsl2.html

```
#　無事にWSL2にUbuntuを導入！
PS C:\Users\USER> wsl -l
Linux 用 Windows サブシステム ディストリビューション:
Ubuntu (既定)
PS C:\Users\USER> wsl -d Ubuntu uname -r
5.15.90.1-microsoft-standard-WSL2
PS C:\Users\USER>
```
## ハードウェア要件の確認
PytorchとTensorFlowはNVIDIAのグラフィックボードがサポートされています。
次のGPUに当てはまるものを揃えると良いパフォーマンスが出ます。
> 次の GPU 対応デバイスがサポートされています。
CUDA® アーキテクチャ 3.5、5.0、6.0、7.0、7.5、8.0 以降を搭載した NVIDIA® GPU カード。

まずは、NVIDIAを動作させるためにドライバーをインストールします。
こちらはWindows側にインストールしていきます。
https://www.nvidia.co.jp/Download/index.aspx?lang=jp

次に、CUDAのバージョンを確認していきましょう。



私のPCは
**GPU:	ZOTAC NVIDIA GeForce RTX4080 (GDDR6X-16GB)**
なので、ハードウェア要件に当てはまっています。
```
nvidia-smi
```
をシェルで実行するとGPUの情報が確認できます。

```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.120                Driver Version: 537.58       CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 4080        On  | 00000000:2B:00.0  On |                  N/A |
| 31%   32C    P8              14W / 320W |   1178MiB / 16376MiB |      2%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A        23      G   /Xwayland                                 N/A      |
+---------------------------------------------------------------------------------------+
```

CUDA Version: 12.2 でした！

このコマンドの詳細については以下の記事を参考になりました！
https://qiita.com/miyamotok0105/items/1b34e548f96ef7d40370

## Pytorchのインストール(GPU使用の場合)
まずPytorchを導入していきましょう。

今回はpipコマンドからライブラリを導入していくため、PythonとpipコマンドをWSL2で使えるようにしておきます。
```
sudo apt -y update
sudo apt -y install python3-dev python3-pip python3-setuptools
```
以下の公式サイトからPytorchのインストールコマンドを選びます。
https://pytorch.org/

インストールできたか確認していきましょう。

```
# PythonでPytorchのバージョンを確認
$ python3 -c "import torch; print( torch.__version__ )"
2.1.0+cu121
```
PytorchでGPUが使用できるか確認します。とても大切。
```
# PytorchでGPUが使用できるか確認
$ python3 -c "import torch; print(torch.__version__, torch.cuda.is_available())"
2.1.0+cu121 True
```
Pytorchの動作確認をしていきます。

```
# python3でPytorchの動作確認
$ python3 -c "import torch; print( torch.__version__ )"
2.1.0+cu121

$ python3 -c "import torch; print(torch.__version__, torch.cuda.is_available())"
2.1.0+cu121 True

$ python3
Python 3.11.6 (main, Oct  2 2023, 13:45:54) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.

>>> import torch
 torch.rand(5, 3)
print(x)>>> x = torch.rand(5, 3)
>>> print(x)
tensor([[0.4083, 0.9996, 0.6138],
        [0.6909, 0.0726, 0.1601],
        [0.4745, 0.9035, 0.0309],
        [0.0565, 0.3769, 0.4328],
        [0.5298, 0.0900, 0.0638]])
```

Pytorchの導入できました！

## TensorFlowのインストール(GPU使用の場合)
次にTensorFlowのインストールをしました。
インストール手順は引き続き、金子先生の記事に沿って行いました。
https://www.kkaneko.jp/tools/wsl/wsl_tensorflow2.html

TensorFLowがインストールできたか確認していきます。
以下はTensorFLowがGPUを認識できているのか確認するコマンドと結果です。

```
~$ python3 -c "from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())"
2023-10-21 22:47:33.485793: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-21 22:47:33.485844: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-21 22:47:33.485871: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-21 22:47:33.489461: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-21 22:47:34.072622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2023-10-21 22:47:34.715020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:47:34.835683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:47:34.835755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:47:35.334747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:47:35.334824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:47:35.334846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2023-10-21 22:47:35.334899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:47:35.334932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 13340 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:2b:00.0, compute capability: 8.9
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 5767509284482495646
xla_global_id: -1
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 13988003840
locality {
  bus_id: 1
  links {
  }
}
incarnation: 581585132380973155
physical_device_desc: "device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:2b:00.0, compute capability: 8.9"
xla_global_id: 416903419
]
```
金子先生の記事によると、
>「device_type: "GPU"」があれば，GPUが認識できている．

とのことなので、安心しました。

## Pythonプログラムを実行してTensorFlowの動作確認
最後に、TensorFlowの動作確認をしていきます。
```
$ python3
Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import tensorflow as tf
2023-10-21 22:50:09.166516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-21 22:50:09.166567: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-21 22:50:09.166595: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-21 22:50:09.170125: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-21 22:50:09.742152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
>>> hello = tf.constant('Hello, TensorFlow!')
2023-10-21 22:50:10.387277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:50:10.398297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:50:10.398370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:50:10.399660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:50:10.399710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:50:10.399749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:50:10.898379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:50:10.898451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.c
[image]c:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:50:10.898481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2023-10-21 22:50:10.898534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-10-21 22:50:10.898569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13340 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:2b:00.0, compute capability: 8.9
>>> print(hello)
tf.Tensor(b'Hello, TensorFlow!', shape=(), dtype=string)```
```
これで無事にTensorFlowがインストールされました。

# まとめ
公式マニュアルからインストールしたらGPUでうまく動作しなかったため、調べた結果、信頼性抜群の最高のインストールマニュアルに出会いました。
福山大学の金子先生、本当に助かりました！！！
https://www.fukuyama-u.ac.jp/eng/information-engineering/kaneko-kunihiko/