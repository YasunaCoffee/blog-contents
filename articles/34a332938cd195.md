---
title: "生成AIパスポート試験でおさえたいAI用語まとめ"
emoji: "🧠"
type: "idea"
topics:
  - "ai"
  - "機械学習"
  - "資格試験"
  - "生成ai"
published: true
published_at: "2023-09-28 23:52"
---

## はじめに
2023年10月6-7日に第１回生成AIパスポート試験が行われます。
まさに学習真っ最中でございます。
https://zenn.dev/yasuna/articles/8bfd5d680bfa78

以下、学習のアウトプットのために生成AIパスポートで知っておきたいAI用語をまとめていきます。
完全な用語集については以下のシラバスからみられます。
https://guga.or.jp/assets/syllabus.pdf

## -AI(人工知能)-
:::message
**ルールベース**
人間が事前に作成したルールや知識をコンピュータプログラムに組み込むことで、それにもとづいて予測や判断を行う。大量のルールの開発やメンテナンスに時間とコストがかかる
:::
:::message
**機械学習**
統計的手法やアルゴリズムを用いて、入力した大量のデータから機械が自分で学習するための技術。また、機械学習でデータのパターンや関係性を把握し表現するモデルのことを「学習済みモデル」という
:::
:::message
**教師あり、学習教師なし学習、強化学習**
機械学習を実現するには大きく分けてこれら３種類の手法がある。
- 教師あり学習...正解データのペアを与えてモデルをトレーニングする
- 教師なし学習...正解データのペアを与えずモデルが自ら発見することでトレーニングする
・似た特徴やパターンを持つグループに分類することをクラスタリングという
・データの次元(変数)を減らすことで、情報を保持しながらデータの特徴を抽出することを次元削減という
- 強化学習...コンピュータに「報酬」という特定の目標を設定し、達成するための最適な行動を学習させる手法。ゲームAIや自動運転車に応用されている
- 半教師あり学習...少量の正解データを用いて大量のラベルのないデータを効率的に学習する方法
:::
:::message
**ノーフリーランチ定理**
1995年に提唱され、どの問題にも万能で汎用的なモデルは存在しないという前提があるという定理。
:::
:::message
**人工ニューロン（ノード）**
神経細胞の仕組みをプログラミングによって再現したもの。
- ニューラルネットワーク...人口ニューロンをいくつかの層に配列し、はじめに提供されたデータを次々に変換して情報を処理する仕組み。1980年代後半にボルツマンマシンが提唱された。
- ディープラーニング...ニューラルネットワークを何層にも重ねて作られたシステム
- 情報の重みづけ...ニューラルネットワークにおけるノードが情報量を調節すること
:::
:::message
**過学習**
訓練データに過剰に適合してしまいうまく予測できなくなる現象
- 正則化...モデルの複雑さを制限すること
- ドロップアウト...ランダムに一部の人口ニューロンを無効化すること
:::
:::message
**転移学習**
一つの問題で学習したモデルの知識を別の問題へ適応する手法
:::

## -生成AI(ジェネレーティブAI)-
:::message
**制約付きボルツマンマシン**
1986年に開発された、ネットワークの入力部分と推定する部分の２つに分かれ学習が収束しやすいモデル
:::
:::message
**自己回帰モデル**
過去のデータを使って次のデータを予測する方法。時系列データの予測に効果的
:::
:::message
**CNN（畳み込みニューラルネットワーク）**
Convolutional Neural Networkはアーキテクチャで、特に画像認識や画像処理に効果的なモデル
:::
:::message
**VAE（変分自己符号化器）**
データの次元削減やデータの生成に使用されるニューラルネットワーク。ノイズが混ざったデータから元データを再現する能力がある
:::
:::message
**GAN（敵対的生成ネットワーク）**
Generative Adversarial Networksは生成器(Generator)と識別器(Discriminator)の２つのネットワークから構成される。Generatorはより本物に近いデータを生成する能力を向上させる。訓練から新しいデータを生成することができる
:::
:::message
**RNN（回帰型ニューラルネットワーク）**
Recurrent Neural Network*は過去の情報を記憶しながら新しい入力を処理するという特性から、時系列データの処理に適している。リカレント層は過去の情報を記憶し、それを現在の情報に判断できる
:::
:::message
**LSTM（長・短期記憶）**
時系列データや文章などのシーケンスデータを処理するための特殊な種類のRNN。
:::
:::message
**Transformerモデル**
2017年にGoogleの研究者が紹介したモデル。Self-Attentionという機能を採用。データの順番に依存せず、全ての要素を一度に考慮に入れて処理できる
- Attention Mechanism...入力された単語の重要度を得点づけてモデルが理解することが可能
- シーケンス全体の並列処理...一度に全ての単語を処理する
- 位置エンコーディング...シーケンス単語の順序が保持される
- エンコーダとデコーダ...エンコーダは入力を連続的な数値表現に変換し、デコーダは最終的な出力を生成する
:::
:::message
**GPTモデル**
Open AIが開発したモデル。Transformerアーキテクチャをベースとした新しい文章を生成するAI
:::
:::message
**BERTモデル**
Googleが開発した自然言語処理のモデル。一部をTransformerをベースとする。双方向性があり、単語の意味が文脈から変わる場合でも意味が捉えられる。
- MLM（Masked Language Model）...語順通りだけではなく、その逆からも読むを実現している。
- NPS(Next Sentence Prediction)...ある文が別の文の直後に来るかどうかを予測すること。
:::
:::message
**RoBERTa、ALBERT（a Lite BERT）**
FacebookがBERTの改良モデルとして発表した。
:::
## まとめ
AIの基礎知識をまとめてみました！
聞いたことがある用語が多かったですが時系列に沿っておさらいできてスッキリしました。
引き続き生成AIパスポート試験やAIトレンドを追っていきたいと思います！